{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Selenium for scraping of infinity pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "* conda install selenium\n",
    "* Install the geckodriver to run firefox in the background\n",
    "\n",
    "https://github.com/mozilla/geckodriver/releases\n",
    "\n",
    "Download the binary for your system and place the binary in the same folder as this Jupyter notebook\n",
    "\n",
    "![images/assets.jpg](images/assets.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#infinityscroll - via Geckodriver, code opens Firefox (needs to be preinstalled) and \n",
    "#10x, it will scroll down to the end of the page, wait 4 seconds and then scroll to the end again\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import NoAlertPresentException\n",
    "import time\n",
    "\n",
    "class Sel():\n",
    "    def __init__(self):\n",
    "        self.driver = webdriver.Firefox()\n",
    "        self.driver.implicitly_wait(30)\n",
    "        self.base_url = \"https://twitter.com\"\n",
    "        self.verificationErrors = []\n",
    "        self.accept_next_alert = True\n",
    "\n",
    "        driver = self.driver\n",
    "        delay = 3\n",
    "        driver.get(self.base_url + \"/search?q=merkel&src=typed_query\")\n",
    "        \n",
    "        #driver.find_element_by_link_text(\"Alle anzeigen\").click()\n",
    "        for i in range(1,10):\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(4)\n",
    "        html_source = driver.page_source\n",
    "        data = html_source.encode('utf-8')\n",
    "        print(html_source)\n",
    "\n",
    "Sel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Selenium for scraping of websites that need a login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example use Selenium for managing the session and bs4 for scraping the data\n",
    "#Selenium also offers the scraping functionalities but they are easier to use in bs4\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://www.linkedin.com/login?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find how the elements are called by checking the website (right-click on button -> Inspect)\n",
    "#there are different ways to identify elements (by_id; by_name; by_xpath)\n",
    "driver.find_element_by_id(\"username\").send_keys(\"tempwebmining@tempr.email\")\n",
    "driver.find_element_by_name(\"session_password\").send_keys(\"webmining\")\n",
    "driver.find_element_by_xpath(\"//button[@data-litms-control-urn='login-submit']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=driver.get(\"https://www.linkedin.com/jobs/search/?keywords=data%20science\")\n",
    "html_source = driver.page_source\n",
    "data = html_source.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(data, 'html')\n",
    "events = soup.findAll('div',{'class':re.compile(\"job-card-container*\")})\n",
    "print(len(events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for e in events:\n",
    "    e1 = e.find('a',{'class':re.compile(\"^job-card-list__title.*\")})\n",
    "    if e1:\n",
    "        print(e1.get_text())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to scroll down on the internal div to get all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#above code was scrolling down the entire page, however, we wanted to only scroll down the the job offering element\n",
    "x=driver.get(\"https://www.linkedin.com/jobs/search/?keywords=data%20science&start=0\")\n",
    "driver.execute_script(\"document.getElementsByClassName('jobs-search-results')[0].scrollTop=2000\")\n",
    "time.sleep(10)\n",
    "html_source = driver.page_source\n",
    "data = html_source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(data, 'html')\n",
    "events = soup.findAll('li',{'class':re.compile(\"jobs-search-results__list-item*\")})\n",
    "print(len(events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in events:\n",
    "    e1 = e.find('a',{'class':re.compile(\"^job-card-list__title.*\")})\n",
    "    if e1:\n",
    "        print(e1.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skeleton code without Selenium (doesn't work with Linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import Session\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "s=Session()\n",
    "site = s.get(\"https://www.linkedin.com/login\")\n",
    "bs_content = bs(site.content, \"html.parser\")\n",
    "loginCsrfParam = bs_content.find(\"input\", {\"name\":\"loginCsrfParam\"})[\"value\"]\n",
    "csrfToken = bs_content.find(\"input\", {\"name\": \"csrfToken\"})['value']\n",
    "sIdString = bs_content.find(\"input\", {\"name\": \"sIdString\"})['value']\n",
    "print(loginCsrfParam)\n",
    "print(csrfToken)\n",
    "print(sIdString)\n",
    "\n",
    "\n",
    "login_data = {\"trk\": \"guest_homepage-basic_sign-in-submit\",\"session_key\":\"tempwebmining@tempr.email\",\"session_password\":\"webmining\", \"loginCsrfParam\":loginCsrfParam, 'csrfToken': csrfToken,'sIdString': sIdString}\n",
    "s.post(\"https://www.linkedin.com/checkpoint/lg/login-submit\",login_data)\n",
    "r = s.get(\"https://www.linkedin.com/jobs/search/?keywords=data%20science\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
