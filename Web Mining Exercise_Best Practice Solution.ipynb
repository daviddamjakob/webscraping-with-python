{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining Exercise_Best Practice Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Go to http://quotes.toscrape.com/ and scrap the following information from the website. The website as several pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![exercise01.PNG](images/exercise01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The quote\n",
    "* The Author\n",
    "* The tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Store the results in a database, retrieve them and create histogram on the number of quotes per author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the two libraries which have to be used for scraping\n",
    "# The request library is used to request information from the internet\n",
    "import requests\n",
    "\n",
    "\n",
    "# BeautifulSoup is a library allowing a code to parse through a html text\n",
    "from bs4 import BeautifulSoup as bsoup \n",
    "\n",
    "# Requesting the website that contains the quotes, to work with it \n",
    "website = requests.get(\"http://quotes.toscrape.com\") \n",
    "\n",
    "# Structure the website content to see how it is build and how the code blocks are structured\n",
    "soup = bsoup(website.content)\n",
    "\n",
    "# Print the structured website content to analyze it \n",
    "soup.prettify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL from which we want to scrape the information\n",
    "url = \"http://quotes.toscrape.com/page/%i/\" \n",
    "\n",
    "# As the website has 10 pages in total we start at page 1 \n",
    "page = 1 \n",
    "\n",
    "# Parameter to stop our crawl when necessary\n",
    "stop = False \n",
    "\n",
    "# List to include scraped info into\n",
    "list1 = [] \n",
    "\n",
    "# Requesting webpage\n",
    "while stop == False:\n",
    "    r = requests.get(\"http://quotes.toscrape.com/page/%i/\" % page)\n",
    "    soup = bsoup(r.content)\n",
    "    \n",
    "   # If the crawler does not find a quote it should stop\n",
    "    if soup.find(\"div\", {\"class\":\"quote\"}) == None:\n",
    "        stop = True\n",
    "        \n",
    "    # If a code is found, the following code is executed    \n",
    "    else:\n",
    "        \n",
    "        # A generator is crawling over the website using the information given in the html-code\n",
    "        # The crawler is looking for specific classes and delimiter for the different categories, which can be seen in the html code: \n",
    "            # A quote has the class \"text\" and has \"span\" as a delimiter\n",
    "            # The author has the class \"author\" and has \"small\" as a delimiter\n",
    "            # The tags have the class \"tag\" and have \"a\" as a delimiter\n",
    "            # As there can be multiple tags for one quote, a second generator is used to extract all tags\n",
    "        for quote in soup.find_all(\"div\", {\"class\":\"quote\"}):\n",
    "            d = {} # dictionary for our scraped information, where the data is stored\n",
    "            d['quote'] = quote.find(\"span\", {\"class\":\"text\"}).text\n",
    "            d['author'] = quote.find(\"small\", \\\n",
    "            {\"class\":\"author\"}).text\n",
    "            tag_list = [tag.text for tag in quote.find_all(\"a\",\\\n",
    "            {\"class\":\"tag\"})]\n",
    "            d['tags'] = ', '.join(tag_list)\n",
    "            \n",
    "            # The crawled data is added to the list as a dictionary\n",
    "            list1.append(d)\n",
    "        \n",
    "        # When one page is crawled the crawler starts with the next page by increasing the page number by one, until no more quotes are found \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data is correctly structured to be imported into a sql-database using pandas dataframe\n",
    "import pandas as pd\n",
    "pd.DataFrame(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change list of dictionaries into a list of tuples to upload it to the database\n",
    "result = [(d['quote'], d['author'], d['tags']) for d in list1]\n",
    "\n",
    "#Show result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-in information\n",
    "\n",
    "# Import pymysql to connect to sql-database\n",
    "import pymysql\n",
    "\n",
    "# Define the information that is required to connect to database\n",
    "host=\"xxx\"\n",
    "user=\"xxxx\"\n",
    "password=\"xxx\"\n",
    "db=\"xxx\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table-creation in sql-database\n",
    "\n",
    "# Connect to sql-database\n",
    "connection = pymysql.connect(host=host,user=user,password=password,db=db)\n",
    "try:\n",
    "    cursorObject            = connection.cursor()  \n",
    "\n",
    "# Add a table to the wm_grp2 database with an unique identifier (id) which increments automatically and create columns for the scraped quotes, authors and tags\n",
    "    sqlCreateTableCommand  = \"CREATE TABLE \"+db+\".myquotes (\"\\\n",
    "        \"id INT NOT NULL AUTO_INCREMENT,\"\\\n",
    "        \"quote VARCHAR(10000) NOT NULL,\"\\\n",
    "        \"author VARCHAR(100) NOT NULL,\"\\\n",
    "        \"tags VARCHAR(100) NOT NULL ,\"\\\n",
    "        \"PRIMARY KEY (id));\"\n",
    "    \n",
    "\n",
    "# Execute the command to create the table in the sql-database\n",
    "    cursorObject.execute(sqlCreateTableCommand)\n",
    "\n",
    "    # Fetch all the rows - from the command output\n",
    "    rows = cursorObject.fetchall()\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "\n",
    "# Define a message that will be shown if the code could not be executed correctly \n",
    "except Exception as e:\n",
    "    print(\"Exeception occured:{}\".format(e))\n",
    "\n",
    "# Close connection to sql-database    \n",
    "finally:\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload of scraped data to the created table in the sql-database\n",
    "\n",
    "# Define an insert query where the values from the list can be passed into the table in the sql-database\n",
    "insert_query = \"INSERT INTO myquotes (quote, author, tags) VALUES (%s, %s, %s)\"\n",
    "\n",
    "# Connect to the sql-database\n",
    "connection = pymysql.connect(host=host,user=user,password=password,db=db)\n",
    "try:\n",
    "    cursorObject = connection.cursor()                                     \n",
    "\n",
    "    #Define the insert query as insertStatement\n",
    "    insertStatement = insert_query\n",
    " \n",
    "    # Using the executemany-statement to upload every element in the list of tuples to the sql-database\n",
    "    cursorObject.executemany(insertStatement,result)\n",
    "    connection.commit();\n",
    "\n",
    "# Define a message that will be shown if the code could not be executed correctly   \n",
    "except Exception as e:\n",
    "    print(\"Exeception occured:{}\".format(e))\n",
    "\n",
    "# Close connection to sql-database\n",
    "finally:\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download of the information that is stored in the table in the sql-database\n",
    "\n",
    "# Connect to the sql-database\n",
    "connection = pymysql.connect(host=host,user=user,password=password,db=db)\n",
    "try:\n",
    "    cursorObject = connection.cursor() \n",
    "    \n",
    "    # Select everything from the table \n",
    "    sqlQuery = \"select * from \"+db+\".myquotes\"\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(sqlQuery)\n",
    "    # Get all rows\n",
    "    records = cursor.fetchall()\n",
    "        \n",
    "    # Print all data\n",
    "    print(records)\n",
    "    \n",
    "    # Print data (structured in different rows) for better overview and to check if all information is included \n",
    "    print(\"\\nPrinting table\")\n",
    "    for row in records:\n",
    "        print(\"ID = \", row[0], )\n",
    "        print(\"Quote = \", row[1], )\n",
    "        print(\"Author = \", row[2])\n",
    "        #\"\\n\" to have space between results\n",
    "        print(\"Tag  = \", row[3], \"\\n\")\n",
    "        \n",
    "# Close connection to sql-database\n",
    "finally:\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the downloaded data\n",
    "\n",
    "# Store the data from the sql-database as a table (dataframe)\n",
    "records_table = pd.DataFrame(records)\n",
    "\n",
    "# Rename the columns\n",
    "records_table.columns = ['ID', 'Quote', 'Author', 'Tags']\n",
    "\n",
    "# Hide the index to improve data-appearance and show the data as a table (dataframe)\n",
    "records_table.style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "records_table['Author'].value_counts().plot(kind='bar',figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
